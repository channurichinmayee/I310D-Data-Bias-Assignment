{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Bias Assignment: Testing Model Fairness for Hate Speech Tweets for two marginalized groups of American Society**\n",
        "\n",
        "**Chinmayee Channuri**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **Aim + Methodology:**\n",
        "\n",
        "In this study, I aimed to utilize the Perspective API and test it with regards to social media hate speech against two marginalized groups of society, Asian Americans and African Americans. Hence, my sensitive attributes were \"Against Asian-Americans\" and \"Against African Americans\".\n",
        "\n",
        "I collected a sample size of 50 tweets (comprising of tweets with hate speech for Asian Americans and African Americans), and marked them with binary identifiers, \"Toxic\" or \"Non-Toxic\" according to how I felt about them, or what my reaction was to them when I read them. These values became my expected values.\n",
        "\n",
        "Then, I utilized the Perspective API to run each tweet text through it to receive a predicted value for the toxicity of the tweet, which was a number in the range of 0-1.\n",
        "\n",
        "I loaded the CSV file with my data in it using Pandas into my Jupyter Notebook to analyze and apply the accuracy study to it.\n",
        "\n",
        "The main transformation I performed for my dataset was when I added a seperate column, \"Predicted Label\" into the data frame to deem a tweet as toxic or non-toxic based on the predicted values from Perspective API.\n",
        "\n",
        "Then, I analysed the Model Fairness from the Predictive Equity standpoint to measure and observe the accuracy performance for each category, Toxic v. Non-Toxic, for each ethnicity group, Asian Americans and African Americans.\n",
        "\n",
        "\n",
        "# **Hypothesis:**\n",
        "\n",
        "I hypothesize that there would be differences between the accuracies for the toxicity levels for hate speech against Asian Americans and African Americans to account for the dataset that Perspective API was potentially trained on."
      ],
      "metadata": {
        "id": "9Mc020UvOlW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **API Call Code**"
      ],
      "metadata": {
        "id": "Yw4j5zriXjgP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPr6PGpHXhqU"
      },
      "outputs": [],
      "source": [
        "from googleapiclient import discovery\n",
        "import json\n",
        "\n",
        "API_KEY = 'AIzaSyD336HZlTbzcDu59k3Vi9Mq14eT0qbBJp0'\n",
        "\n",
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey=API_KEY,\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  static_discovery=False,\n",
        ")\n",
        "\n",
        "analyze_request = {\n",
        "  'comment': { 'text': \"\" },\n",
        "  'requestedAttributes': {'TOXICITY': {}}\n",
        "}\n",
        "\n",
        "response = client.comments().analyze(body=analyze_request).execute()\n",
        "print(json.dumps(response, indent=2))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloaded the table as a CSV and loaded it here via Numpy and Pandas**"
      ],
      "metadata": {
        "id": "_D7AGEh7E9Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "test_df = pd.read_csv(\"data-bias-racism-tweets-dataset.csv\")\n",
        "test_df\n",
        "\n",
        "# change expecyted score to expected label\n",
        "# add another column into the csv\n",
        "# loop over preditced scores and create a list over the predicted label make it toxic vs non toxic\n",
        "\n",
        "def create_label(score):\n",
        "  if score > 0.5:\n",
        "    return \"Toxic\"\n",
        "  else:\n",
        "    return \"Non Toxic\"\n",
        "\n",
        "test_df['Predicted Label'] = test_df['Predicted Score'].apply(create_label)\n",
        "test_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8oqlyEgwXqpy",
        "outputId": "cf63e512-a18b-43ca-cb11-3644ee989baf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Sensitive Attribute  \\\n",
              "0     Against Asian Americans   \n",
              "1     Against Asian Americans   \n",
              "2     Against Asian Americans   \n",
              "3     Against Asian Americans   \n",
              "4     Against Asian Americans   \n",
              "5     Against Asian Americans   \n",
              "6     Against Asian Americans   \n",
              "7     Against Asian Americans   \n",
              "8     Against Asian Americans   \n",
              "9     Against Asian Americans   \n",
              "10    Against Asian Americans   \n",
              "11    Against Asian Americans   \n",
              "12    Against Asian Americans   \n",
              "13    Against Asian Americans   \n",
              "14    Against Asian Americans   \n",
              "15    Against Asian Americans   \n",
              "16    Against Asian Americans   \n",
              "17    Against Asian Americans   \n",
              "18    Against Asian Americans   \n",
              "19    Against Asian Americans   \n",
              "20    Against Asian Americans   \n",
              "21    Against Asian Americans   \n",
              "22    Against Asian Americans   \n",
              "23    Against Asian Americans   \n",
              "24    Against Asian Americans   \n",
              "25    Against Asian Americans   \n",
              "26    Against Asian Americans   \n",
              "27    Against Asian Americans   \n",
              "28  Against African Americans   \n",
              "29  Against African Americans   \n",
              "30  Against African Americans   \n",
              "31  Against African Americans   \n",
              "32  Against African Americans   \n",
              "33  Against African Americans   \n",
              "34  Against African Americans   \n",
              "35  Against African Americans   \n",
              "36  Against African Americans   \n",
              "37  Against African Americans   \n",
              "38  Against African Americans   \n",
              "39  Against African Americans   \n",
              "40                        NaN   \n",
              "41  Against African Americans   \n",
              "42  Against African Americans   \n",
              "43  Against African Americans   \n",
              "44  Against African Americans   \n",
              "45  Against African Americans   \n",
              "46  Against African Americans   \n",
              "47  Against African Americans   \n",
              "48  Against African Americans   \n",
              "49  Against African Americans   \n",
              "\n",
              "                                                 Text Expected Score  \\\n",
              "0   #COVID19 so this all started cuz some chinks w...          Toxic   \n",
              "1   Hey China thanks for literally fucking up the ...          Toxic   \n",
              "2   Yeah no shit, it only took the mainstream medi...          Toxic   \n",
              "3   @LeoVaradkar Why is he licking China‚Äôs boots. ...          Toxic   \n",
              "4   If you‚Äôre just now learning that China fudges ...          Toxic   \n",
              "5   CCP China doing all types of shady &amp; evil ...          Toxic   \n",
              "6   @PockyParade Are you retarded? China lied and ...          Toxic   \n",
              "7   President Trump was called a racist calling co...          Toxic   \n",
              "8   #ChinaVirus when will these dumbasses learn fr...         Toxic    \n",
              "9   I AGREE WITH @SenTomCotton THE CORONAVIRUS MOR...          Toxic   \n",
              "10  @skaterjeongyeon @ti_xu_dao Obviously the majo...          Toxic   \n",
              "11  China is going to kill America one way or anot...          Toxic   \n",
              "12  You‚Äôre just mad because you maybe can‚Äôt use th...          Toxic   \n",
              "13  coronavirus is not xenophobic or racist and wi...      Non Toxic   \n",
              "14  STOP CALLING COVID-19 THE ‚ÄúCHINESE VIRUS‚Äù WHAT...      Non Toxic   \n",
              "15  @SenKamalaHarris @GOPLeader Hey Kevin - Callin...      Non Toxic   \n",
              "16  Being racist against the Chinese won't stop th...      Non Toxic   \n",
              "17  Put a #STOP üõë to all this racism Asians have b...      Non Toxic   \n",
              "18  STOP FUCKING CALLING IT THE 'CHINESE VIRUS' yo...      Non Toxic   \n",
              "19  Why was it so normal for people to call me a c...      Non Toxic   \n",
              "20  I am SICK üëè OF üëè IT üëè AND SO IS EVERY ASIAN ou...      Non Toxic   \n",
              "21  I am tweeting to Support #IAmNotAVirus hashtag...      Non Toxic   \n",
              "22  STOP CALLING COVID-19 THE ‚ÄúCHINESE VIRUS‚Äù THE ...      Non Toxic   \n",
              "23  Shame on you if you attack Asians over this vi...      Non Toxic   \n",
              "24  My husband continues to deliberately call it t...      Non Toxic   \n",
              "25  @gabewildau It's called preventative actions. ...      Non Toxic   \n",
              "26  Discrimination is not okay no matter what the ...      Non Toxic   \n",
              "27  We are not diseased. We are not scapegoats. We...      Non Toxic   \n",
              "28  #BlackLivesMatter are a bunch of #terrorists, ...          Toxic   \n",
              "29  BlackLivesMatter Wake Up you are Pawns Being u...          Toxic   \n",
              "30  Three feral black teens kill elderly white man...         Toxic    \n",
              "31  Black lives matter is the biggest rasist organ...         Toxic    \n",
              "32  Congratulations to George Floyd on being drug ...          Toxic   \n",
              "33  BLM Protestors horribly harass elderly Pittsbu...          Toxic   \n",
              "34  3 Dallas Cops killed, 7 wounded. this is now w...          Toxic   \n",
              "35  Black people beating whites when a thug got sh...          Toxic   \n",
              "36  BLM is pathetic once again! Obama you are path...         Toxic    \n",
              "37  These THUGS are dishonoring the memory of Geor...          Toxic   \n",
              "38  Meet the new KKK, they call themselves \"Black ...          Toxic   \n",
              "39  #BlackLivesMatter This should not be a controv...     Non Toxic    \n",
              "40  Uber stands in solidarity with the Black commu...     Non Toxic    \n",
              "41  #BlackLivesMatter isn't a hate group. We don't...     Non Toxic    \n",
              "42  Our voice would be nothing without Black cultu...      Non Toxic   \n",
              "43  enough is enough. it's time we stand up for th...      Non Toxic   \n",
              "44  no more of this. no more. i'm sick of black pe...     Non Toxic    \n",
              "45  Racism is a 400-year pandemic. Enough is enoug...      Non Toxic   \n",
              "46  There's no such thing as BLUE LIVES... Stop co...     Non Toxic    \n",
              "47  Our greatest strength. Black lives matter. Whi...      Non Toxic   \n",
              "48  declaration: black bodies will no longer be sa...     Non Toxic    \n",
              "49  When are black people going to wake up and rea...     Non Toxic    \n",
              "\n",
              "    Predicted Score Predicted Label  \n",
              "0          0.471199       Non Toxic  \n",
              "1          0.757031           Toxic  \n",
              "2          0.658813           Toxic  \n",
              "3          0.674409           Toxic  \n",
              "4          0.654470           Toxic  \n",
              "5          0.765238           Toxic  \n",
              "6          0.630852           Toxic  \n",
              "7          0.338998       Non Toxic  \n",
              "8          0.469820       Non Toxic  \n",
              "9          0.471199       Non Toxic  \n",
              "10         0.630852           Toxic  \n",
              "11         0.473235       Non Toxic  \n",
              "12         0.767545           Toxic  \n",
              "13         0.301366       Non Toxic  \n",
              "14         0.440214       Non Toxic  \n",
              "15         0.507899           Toxic  \n",
              "16         0.323975       Non Toxic  \n",
              "17         0.292228       Non Toxic  \n",
              "18         0.944597           Toxic  \n",
              "19         0.401485       Non Toxic  \n",
              "20         0.602549           Toxic  \n",
              "21         0.057748       Non Toxic  \n",
              "22         0.902923           Toxic  \n",
              "23         0.588517           Toxic  \n",
              "24         0.686364           Toxic  \n",
              "25         0.502472           Toxic  \n",
              "26         0.111713       Non Toxic  \n",
              "27         0.233793       Non Toxic  \n",
              "28         0.464518       Non Toxic  \n",
              "29         0.601528           Toxic  \n",
              "30         0.717606           Toxic  \n",
              "31         0.720280           Toxic  \n",
              "32         0.320209       Non Toxic  \n",
              "33         0.588517           Toxic  \n",
              "34         0.588517           Toxic  \n",
              "35         0.836470           Toxic  \n",
              "36         0.767545           Toxic  \n",
              "37         0.416905       Non Toxic  \n",
              "38         0.321285       Non Toxic  \n",
              "39         0.151034       Non Toxic  \n",
              "40         0.142560       Non Toxic  \n",
              "41         0.170252       Non Toxic  \n",
              "42         0.326609       Non Toxic  \n",
              "43         0.168519       Non Toxic  \n",
              "44         0.473235       Non Toxic  \n",
              "45         0.372227       Non Toxic  \n",
              "46         0.364568       Non Toxic  \n",
              "47         0.450906       Non Toxic  \n",
              "48         0.360951       Non Toxic  \n",
              "49         0.686364           Toxic  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3762b189-811d-47f6-81f9-4f3670241436\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sensitive Attribute</th>\n",
              "      <th>Text</th>\n",
              "      <th>Expected Score</th>\n",
              "      <th>Predicted Score</th>\n",
              "      <th>Predicted Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>#COVID19 so this all started cuz some chinks w...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.471199</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>Hey China thanks for literally fucking up the ...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.757031</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>Yeah no shit, it only took the mainstream medi...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.658813</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>@LeoVaradkar Why is he licking China‚Äôs boots. ...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.674409</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>If you‚Äôre just now learning that China fudges ...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.654470</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>CCP China doing all types of shady &amp;amp; evil ...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.765238</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>@PockyParade Are you retarded? China lied and ...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.630852</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>President Trump was called a racist calling co...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.338998</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>#ChinaVirus when will these dumbasses learn fr...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.469820</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>I AGREE WITH @SenTomCotton THE CORONAVIRUS MOR...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.471199</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>@skaterjeongyeon @ti_xu_dao Obviously the majo...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.630852</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>China is going to kill America one way or anot...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.473235</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>You‚Äôre just mad because you maybe can‚Äôt use th...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.767545</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>coronavirus is not xenophobic or racist and wi...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.301366</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>STOP CALLING COVID-19 THE ‚ÄúCHINESE VIRUS‚Äù WHAT...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.440214</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>@SenKamalaHarris @GOPLeader Hey Kevin - Callin...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.507899</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>Being racist against the Chinese won't stop th...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.323975</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>Put a #STOP üõë to all this racism Asians have b...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.292228</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>STOP FUCKING CALLING IT THE 'CHINESE VIRUS' yo...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.944597</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>Why was it so normal for people to call me a c...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.401485</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>I am SICK üëè OF üëè IT üëè AND SO IS EVERY ASIAN ou...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.602549</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>I am tweeting to Support #IAmNotAVirus hashtag...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.057748</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>STOP CALLING COVID-19 THE ‚ÄúCHINESE VIRUS‚Äù THE ...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.902923</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>Shame on you if you attack Asians over this vi...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.588517</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>My husband continues to deliberately call it t...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.686364</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>@gabewildau It's called preventative actions. ...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.502472</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>Discrimination is not okay no matter what the ...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.111713</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Against Asian Americans</td>\n",
              "      <td>We are not diseased. We are not scapegoats. We...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.233793</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>#BlackLivesMatter are a bunch of #terrorists, ...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.464518</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>BlackLivesMatter Wake Up you are Pawns Being u...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.601528</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>Three feral black teens kill elderly white man...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.717606</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>Black lives matter is the biggest rasist organ...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.720280</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>Congratulations to George Floyd on being drug ...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.320209</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>BLM Protestors horribly harass elderly Pittsbu...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.588517</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>3 Dallas Cops killed, 7 wounded. this is now w...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.588517</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>Black people beating whites when a thug got sh...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.836470</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>BLM is pathetic once again! Obama you are path...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.767545</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>These THUGS are dishonoring the memory of Geor...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.416905</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>Meet the new KKK, they call themselves \"Black ...</td>\n",
              "      <td>Toxic</td>\n",
              "      <td>0.321285</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>#BlackLivesMatter This should not be a controv...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.151034</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Uber stands in solidarity with the Black commu...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.142560</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>#BlackLivesMatter isn't a hate group. We don't...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.170252</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>Our voice would be nothing without Black cultu...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.326609</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>enough is enough. it's time we stand up for th...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.168519</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>no more of this. no more. i'm sick of black pe...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.473235</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>Racism is a 400-year pandemic. Enough is enoug...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.372227</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>There's no such thing as BLUE LIVES... Stop co...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.364568</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>Our greatest strength. Black lives matter. Whi...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.450906</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>declaration: black bodies will no longer be sa...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.360951</td>\n",
              "      <td>Non Toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>Against African Americans</td>\n",
              "      <td>When are black people going to wake up and rea...</td>\n",
              "      <td>Non Toxic</td>\n",
              "      <td>0.686364</td>\n",
              "      <td>Toxic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3762b189-811d-47f6-81f9-4f3670241436')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3762b189-811d-47f6-81f9-4f3670241436 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3762b189-811d-47f6-81f9-4f3670241436');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fafebac9-84e6-415a-bbbf-c863dee1e0ef\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fafebac9-84e6-415a-bbbf-c863dee1e0ef')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fafebac9-84e6-415a-bbbf-c863dee1e0ef button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Assign a row a binary factor deeming it either toxic, 1, or non-toxic, 0, for both expected score (Toxic/Non-Toxic) and predicted score (>0.5/<0.5) columns**"
      ],
      "metadata": {
        "id": "jjHrLJn2FNy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_actual = [1 if y.strip() == 'Toxic' else 0 for y in test_df['Expected Score']]\n",
        "y_predicted = [1 if y.strip() == 'Toxic' else 0 for y in test_df['Predicted Label']]"
      ],
      "metadata": {
        "id": "HHUX4fEbkmai"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Measured Accuracy of Model via Pickle and SkLearn Machine Learning Models**"
      ],
      "metadata": {
        "id": "hJvtX7dMFFlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_predicted, y_actual)\n",
        "\n",
        "\n",
        "print (f\"Accuracy of the classifier = {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWo0bd7Zx6My",
        "outputId": "ab19b070-e539-4c3e-dbc0-9d906efbdb94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the classifier = 0.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Analyzing the Accuracy of the Classifier:**\n",
        "\n",
        "The overall accuracy of the model is moderatly high, at 66%. But, further analysis must be conducted to understand if the model is able to take into consideration the semantics, mood,message, and overall tone of the tweet rather than just taking the words used at face value and mapping them to being toxic or non-toxic."
      ],
      "metadata": {
        "id": "JTH9XH68FqMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "race_column = test_df[\"Sensitive Attribute\"]\n",
        "asian_indices = []\n",
        "african_american_indices = []\n",
        "\n",
        "for ele in range(len(race_column)):\n",
        "    if race_column[ele] == \"Against Asian Americans\":\n",
        "      asian_indices.append(ele)\n",
        "    else:\n",
        "      african_american_indices.append(ele)\n",
        "\n",
        "y_actual_asian = [y_actual[i] for i in asian_indices]\n",
        "y_predicted_asian = [y_predicted[i] for i in asian_indices]\n",
        "\n",
        "y_actual_african_american = [y_actual[i] for i in african_american_indices]\n",
        "y_predicted_african_american = [y_predicted[i] for i in african_american_indices]\n",
        "\n",
        "print(\"Number of Asian indices:\", len(asian_indices))\n",
        "print(\"Number of African American indices:\", len(african_american_indices))\n",
        "\n",
        "print(y_actual_asian)\n",
        "print(y_predicted_asian)\n",
        "print(y_actual_african_american)\n",
        "print(y_predicted_african_american)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0lKuKmuyDVv",
        "outputId": "ea8e36b9-7b28-48b6-a339-91bdb5df3943"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Asian indices: 28\n",
            "Number of African American indices: 22\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def class_wise_acc(y_actual, y_predicted):\n",
        "    total_p = 0\n",
        "    total_n = 0\n",
        "    TP=0\n",
        "    TN=0\n",
        "    for i in range(len(y_predicted)):\n",
        "        if y_actual[i]==1:\n",
        "            total_p = total_p+1\n",
        "            if y_actual[i]==y_predicted[i]:\n",
        "               TP=TP+1\n",
        "        if y_actual[i]==0:\n",
        "            total_n=total_n+1\n",
        "            if y_actual[i]==y_predicted[i]:\n",
        "               TN=TN+1\n",
        "    return(TP/total_p, TN/total_n)\n",
        "\n",
        "class_1_acc_asian, class_0_acc_asian = class_wise_acc(y_actual_asian, y_predicted_asian)\n",
        "class_1_acc_african_american, class_0_acc_african_american = class_wise_acc(y_actual_african_american, y_predicted_african_american)\n",
        "\n",
        "print (f\"Category 1 Toxic Comments accuracy for Asians = {class_1_acc_asian}\")\n",
        "print (f\"Category 2 Non Toxic Comments accuracy for Asians = {class_0_acc_asian}\")\n",
        "print (f\"Category 3 Toxic Comments accuracy for African-Americans = {class_1_acc_african_american}\")\n",
        "print (f\"Category 4 Non Toxic Comments accuracy for African-Americans = {class_0_acc_african_american}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78q6b1aAzqQs",
        "outputId": "f2616974-c7dc-42b9-dad8-7bacd28d777c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category 1 Toxic Comments accuracy for Asians = 0.6153846153846154\n",
            "Category 2 Non Toxic Comments accuracy for Asians = 0.5333333333333333\n",
            "Category 3 Toxic Comments accuracy for African-Americans = 0.6363636363636364\n",
            "Category 4 Non Toxic Comments accuracy for African-Americans = 0.9090909090909091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Insights**\n",
        "\n",
        "If we compare how close the accuracy values are for both Asians and African-Americans for the Toxic Comments category, we can see that they are roughly close. This means that if a comment is toxic, the model can predict it's toxic with a 61-63% acccuracy.\n",
        "\n",
        "But what is troubling here with the Perspective API usage is the difference in accuracies for the Non-Toxic catgeories for Asians and African-Americans. For African-Americans, the accuracy is at a steep 90%, but for Asians, it's only 53%. This difference indicates that the model is able to recognize toxicity for hate speech against African Americans over that against Asians.\n",
        "\n"
      ],
      "metadata": {
        "id": "7AxhpvSV56q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Detailed Evaluation for Asian American Group**\n",
        "\n",
        "**To identify why there is a discrepancy just for the Non-Toxic comments for Asian Americans**\n",
        "\n",
        "It was surprising to see 61.5% accuracy for the Toxic category for Asian Americans, since most if not all tweets in the dataset were offensive. But, the Perspective API model was not able to detect this due to any subtleties in language, or. Something that could've helped us to delve deeper into understanding the real reasons as to why the accuracies were lower is if the Perspective API model had algorthmic transparency. If us users were given access to the data that the API model was trained/tested on would give us more context.  \n",
        "\n",
        "\n",
        "The accuracy for the Non-Toxic comments for Asians indicated to only be about 50% correct, because of how some tweeters decided to use swear words and profanity to express their frustration about the racist situation post COVID-19 outbreak in the US but they weren't trying to demonstrate any hate towards Asians. This shows that the Perspective API model is weak in understanding tone and mood of the semantics used in statements. In these tweets, they were using toxic vocabulary, like profane language, due to which the API automatically flagged them as \"Toxic\" but are really not and rather are showing support and defending the Asian American population on Twitter.\n",
        "\n"
      ],
      "metadata": {
        "id": "n_MX6h2mJT3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion + Limitations + Scope for Further Research**\n",
        "\n",
        "Based on our study conducted, the analysis we performed, and insights we drew from it, we can conclude that the scope for th ePerspective API being able to identify or demarcate differences in semantics, mood, message, and overall tone of the claim that is inputted is quite limited. This was shown by how the accuracy values were extremely different for the Non-Toxic category in specific for Asian Americans and African Americans. This bias in identifying toxicity for African Americans more than Asian Americans propels us to think about the potential limitations of the model.\n",
        "\n",
        "As identified above, the main limitation of the Perspective API model is that we don't have much context on the dataset it was trained on. There is not much Algorithmic Transparency here, as us users do not have access to the dataset that the model was trained on. This context would help us understand why there was a difference of 40% of accuracy between the Non-toxic accuracy for both marginalized groups.\n",
        "\n",
        "The scope for further research is large here, since this study uncovered great insights into racial differenciation between two different set of marginalized groups in social media. Apart from increasing the sample size, one of the opportunities that can be explored is considering other social media other than Twitter like Facebook, Instagram, and etc. Potentially, we can compare the results obtained for Tweets vs. Facebook posts and understand whether there are more factors affecting it. Another method to extend this research is considering the political side of it, since political arguments are very common in the disucssion of discrimination against marginalized groups in the US. These methods would both equally be interesting and intriguing to explore."
      ],
      "metadata": {
        "id": "7BudvGRAHKiL"
      }
    }
  ]
}